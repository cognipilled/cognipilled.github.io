<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Levi Finkelstein" />
  <title>Information</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../../index.css">
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Information</h1>
<p class="author">Levi Finkelstein</p>
<p class="date">04 09 2021</p>
</header>
<h4 id="i.">I.</h4>
<p>Imagine that you’re a detective faced with a murder and a list of
<span class="math inline">\(16\)</span> suspects each whom is equally
likely to have done it prior to you starting your investigation.</p>
<p>When you find a clue indicating that the culprit is a woman you can
remove the <span class="math inline">\(8\)</span> male suspects, leaving
you with <span class="math inline">\(8\)</span> female suspects. How do
you quantify how much information this clue gave you? One way is to
specify the fraction by which it scaled the search space, which in this
case was by a half: <span class="math display">\[\frac{1}{2}*16 =
8\]</span> So <span class="math inline">\(\frac{1}{2}\)</span> was the
information gain. This means you need <span
class="math inline">\(\frac{1}{16}\)</span> information in total to find
the murderer, i.e. to reduce the search space from <span
class="math inline">\(16\)</span> to <span
class="math inline">\(1\)</span>.</p>
<p>Another way to specify this is to talk about how many times you
require the search space be halved. In this case: <span
class="math display">\[(\frac{1}{2})^N*16 = 1 \Rightarrow N=4\]</span>
So you need <span class="math inline">\(4\)</span> halvings.</p>
<p>An equivalent way of saying this is that you need <span
class="math inline">\(4\)</span> bits. The reason it makes sense to
equate bits to halvings is that a bit can only take on two values, so
each of <span class="math inline">\(1\)</span> and <span
class="math inline">\(0\)</span> would exclude each their own equally
sized disjoint sections of the search space (for example <span
class="math inline">\(0=men\)</span> and <span
class="math inline">\(1=women\)</span>). <strong>If you learn 1 bit of
information you reduce the search space by a half.</strong></p>
<p>In general if there are <span class="math inline">\(S\)</span>
suspects you need clues worth a total of <span
class="math inline">\(N=\log_2{S}\)</span> bits of information since
<span class="math display">\[(\frac{1}{2})^N*S=1 \Rightarrow
(\frac{1}{2})^N=\frac{1}{S}\Rightarrow 2^N=S\]</span> <span
class="math display">\[\Rightarrow \log_2{2^N}=\log_2{S} \Rightarrow N =
\log_2{S}\]</span></p>
<p>Conversely, since <span class="math inline">\(2^N = S\)</span> you
can also think of <span class="math inline">\(N\)</span> (the number of
bits) as how many doublings you need to go from <span
class="math inline">\(1\)</span> to the size of the initial search space
<span class="math inline">\(S\)</span>.</p>
<h4 id="ii.">II.</h4>
<p>In this case the <em>probability</em> of a suspect being the murderer
is equal to the fraction they take up in the current search space. So
before any clues were discovered each on of them had a probability <span
class="math inline">\(p=\frac{1}{16}\)</span> of being guilty. After
discovering the first clue the men were excluded from the search space,
receiving no fraction of it (<span class="math inline">\(p=0\)</span>),
with the women receiving a bigger fraction <span
class="math inline">\(p=\frac{1}{8}\)</span> of the remaining space.</p>
<p>From this you can see that <strong>the more information you gain the
more probability mass gets concentrated in a smaller space</strong>.
When all the probability mass gets concentrated at a single point (<span
class="math inline">\(p=1\)</span> that a single person is the murderer)
you’ve gained all the information you can.</p>
<p>In a converse way, the more information there is (to gain?) the more
probability mass is spread out. This way to think about information
involves considering how many times (how many bits) you need to take
half the probability mass and concentrate it on another half until you
end up with everything on single point.</p>
<p>Before finding the first clue the probability of the guilty being a
woman was <span class="math inline">\(p=\frac{1}{2}\)</span>. So
encountering an event that <em>used</em> to be <span
class="math inline">\(p=\frac{1}{2}\)</span> means now setting it to
<span class="math inline">\(p=1\)</span> by stealing all the probability
mass that’s outside the event, in this case the <span
class="math inline">\(\frac{1}{2}\)</span> of mass contained in the "the
murderer is a man" event.</p>
<p>The number of bits gained by learning a probability <span
class="math inline">\(p\)</span> fact is then equal to <span
class="math display">\[\log_2{\frac{1}{p}}=\log_2{p^{-1}}=-\log_2{p}\]</span>
Because:</p>
<ol>
<li><p><span class="math inline">\(p\)</span> is the fraction by which
you’re scaling the search space (the fraction of the space you’re
cramming all the probability mass into).</p></li>
<li><p><span class="math inline">\(\frac{1}{p}\)</span> is how many
times smaller the search space is (how many times smaller the space
containing all the probability mass is).</p></li>
<li><p>The <span class="math inline">\(N\)</span> in <span
class="math inline">\(2^N=\frac{1}{p}\)</span> is then how many times
you need to cut the search space in two for it to be <span
class="math inline">\(\frac{1}{p}\)</span> times smaller (how many times
you need to relocate half the probability mass to fit it all in a space
<span class="math inline">\(\frac{1}{p}\)</span> times
smaller).</p></li>
<li><p>So <span class="math inline">\(N=\log_2{\frac{1}{p}} =\)</span>
the number of bits gained.</p></li>
</ol>
<h4 id="iii.">III.</h4>
<p>The expected value of a random variable <span
class="math inline">\(X\)</span> with distribution <span
class="math inline">\(P\)</span> is <span
class="math display">\[E(X)=\sum_{i}{}x_i P(x_i)\]</span> If we treat
the number of bits <span
class="math inline">\(\log_2{\frac{1}{p}}\)</span> of information gained
as a random variable, then we have a probability <span
class="math inline">\(p\)</span> of gaining <span
class="math inline">\(\log_2{\frac{1}{p}}\)</span> bits of information
from a random sample of this distribution.</p>
<p>This is what <em>information entropy</em> is: the expected amount of
information gained from a single sample of a probability distribution
(the expected number of times you need to relocate half of the
probability mass to fit it all within the space of the sampled event):
<span class="math display">\[H(X)=E(\log_2{\frac{1}{p}})=\sum_{i}
p_i\log_2{\frac{1}{p_i}}=-\sum_{i} p_i\log_2{p_i}\]</span></p>
<p>Here are some distributions: <span
class="math display">\[D_1=\{p_1=1\}\]</span> <span
class="math display">\[D_2=\{p_1=0.5, p_2=0.5\}\]</span> <span
class="math display">\[D_3=\{p_1=0.5, p_2=0.2, p_3=0.3\}\]</span></p>
<ul>
<li><p>The entropy of <span class="math inline">\(D_1\)</span> is <span
class="math inline">\(0\)</span> since all the probability mass is
already concentrated at a single point.</p></li>
<li><p>The entropy of <span class="math inline">\(D_2\)</span> is <span
class="math inline">\(1\)</span> since you you’re guaranteed to sample a
<span class="math inline">\(p=0.5\)</span> event which needs to relocate
half the probability mass once to concentrated it all at a single
point.</p></li>
<li><p>The entropy of <span class="math inline">\(D_3\)</span> is <span
class="math display">\[0.5 \log_2{\frac{1}{0.5}} + 0.2
\log_2{\frac{1}{0.2}} + 0.3 \log_2{\frac{1}{0.3}}\approx 1.485\]</span>
If you imagine <span class="math inline">\(D_3\)</span> as a
distribution of possible <em>clues</em> you might discover in your
murder investigation, then on average you expect learning <span
class="math inline">\(1.485\)</span> bits of information from each clue,
or equivalently scaling the number of remaining suspects by a factor
<span class="math inline">\((\frac{1}{2})^{1.485}\)</span>. Meaning that
you would on average need a little under <span
class="math inline">\(3\)</span> clues to find the murderer (to scale
the search space down to <span
class="math inline">\(1\)</span>).</p></li>
</ul>
<h4 id="iiii.">IIII.</h4>
<p>There’s something peculiar about the fact that in <span
class="math inline">\(p\log_2{\frac{1}{p}}\)</span>, <span
class="math inline">\(p\)</span> both determines how likely the event is
and how much information you gain from observing it. But if you think
back to the <span class="math inline">\(16\)</span> suspects with equal
probability of being guilty you can kind of see how information gain and
probability are two sides of the same coin: the probability <span
class="math inline">\(p=\frac{1}{16}\)</span> was derived from how much
space any single person took up in the search space, and how much
information you gain from learning that any one person is the murderer
is exactly equal to how much you scale the search space, which is
exactly the how much space they take up in the search space.</p>
<p>This idea of scaling a search space (concentrating probability mass)
is fundamentally what information is. Shannon said</p>
<blockquote>
<p>The fundamental problem of communication is that of reproducing at
one point either exactly or approximately a message selected at another
point. Frequently the messages have meaning; that is they refer to or
are correlated according to some system with certain physical or
conceptual entities. These semantic aspects of communication are
irrelevant to the engineering problem. The significant aspect is that
the actual message is one selected from a set of possible messages. The
system must be designed to operate for each possible selection, not just
the one which will actually be chosen since this is unknown at the time
of design.</p>
</blockquote>
<p>If you have an infinite set of messages that your recipient is
supposed to search through then you need an infinite number of bits to
specify exactly which one you’re sending.</p>
<p>Being a detective trying to figure out who did it is from an
information theoretical point view the same thing as understanding a
message sent from your friend. Before reading the message there’s a
space of possible things your friend might be meaning to communicate
with you. Before finding any clues there’s a space of possible suspects
who might’ve done it. When reading the message, every bit of information
reduces the space by half until you’ve pinpointed to a satisfactory
level of certainty what’s it’s supposed to say. When finding clues, the
space of suspects reduces by half for every bit gained until it’s down
to a single person.</p>
<p>Speaking more generally: when interfacing with the world you gain
information about it (reduce the possibility space) by deleting the
possible world that are incompatible with your observations.
(realistically you would likely do this in a Bayesian way by updating
probability distributions, with the prior space of possible worlds being
a prior distribution).</p>
<h4 id="random-notes"><em>Random notes</em></h4>
<ul>
<li><p>The entropy of a distribution can be interpreted as a measure of
how much information it contains (how much information it can give you
before it runs out of information).</p></li>
<li><p>The conditional entropy <span
class="math inline">\(H(X|Y)\)</span> can be thought of as the
information left in the distribution of <span
class="math inline">\(X\)</span> after an event <span
class="math inline">\(Y\)</span> has happened. For example the entropy
remaining in the distribution of suspects after learning that the guilty
is a woman.</p></li>
<li><p>The information gain of an event can be defined as the difference
between the entropy of the distribution minus the entropy of the
distribution conditioned on the event: <span
class="math inline">\(IG(X|A) = H(X) - H(X|A)\)</span>. This makes sense
if you think about the conditional entropy <span
class="math inline">\(H(X|A)\)</span> as being a measure of how much
information is left in the distribution after the event <span
class="math inline">\(A\)</span> happened (<span
class="math inline">\(IG(X|A)\)</span> is how much information X lost
and you gained). If <span class="math inline">\(A\)</span> is a random
variable (for example the answer to a yes/no question then) then you
need to average over all possible values of <span
class="math inline">\(A\)</span>: <span class="math display">\[H(X|A) =
P(A=yes)H(X|A=yes) + P(A=no)H(X|A=no)\]</span></p></li>
<li><p>In the binary search algorithm you gain <span
class="math inline">\(1\)</span> bit of information about the position
of what you’re searching for in each iteration thus making it have
runtime <span class="math inline">\(O(\log_2{N})\)</span> since that’s
how many times you have to half the search space.</p></li>
<li><p>The entropy of a uniform distribution with <span
class="math inline">\(N\)</span> events is <span
class="math inline">\(\log_2{N}\)</span> for the same reason you need
<span class="math inline">\(\log_2{N}\)</span> bits of worth of clues to
find a murderer out of <span class="math inline">\(N\)</span> equally
suspicious suspects.</p></li>
<li><p>A dictionary with symbols whose frequency follows some
probability distribution is more difficult to compress the higher the
entropy of the distribution, i.e an optimal encoding of the symbols
requires more bits on average to transmit a message constructed from the
dictionary.</p></li>
<li><p>When training decision trees a nice strategy is to place yes/no
questions on each branch that maximize the expected information gain
<span class="math inline">\(IG(X|question)\)</span> given a probability
distribution of <span class="math inline">\(X\)</span> defined by
frequencies of features in the training data. This means you should ask
questions that tends to continuously bisect the training data.</p></li>
<li><p>Joint entropy is the entropy analogue of a joint distribution.
<span class="math display">\[H(X,Y) = H(X) + H(Y|X) = H(Y) +
H(X|Y)\]</span> Which if <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> are independent is <span
class="math inline">\(H(Y) + H(X)\)</span> since <span
class="math inline">\(H(X|Y) = H(X)\)</span>. This makes sense because
independence means you shouldn’t gain any information about the one when
learning about the other. The information entropy left in <span
class="math inline">\(X\)</span> after conditioning on <span
class="math inline">\(Y\)</span> is the same as before
conditioning.</p></li>
<li><p>The mutual information <span
class="math inline">\(I(X;Y)\)</span> of <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> is the information that’s contained in
both <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>. This means you learn <span
class="math inline">\(I(X;Y)\)</span> both when you learn <span
class="math inline">\(X\)</span> and when you learn <span
class="math inline">\(Y\)</span>. <span
class="math inline">\(H(X|Y)\)</span> is the information left in <span
class="math inline">\(X\)</span> after learning <span
class="math inline">\(Y\)</span> so <span class="math inline">\(H(X|Y) =
H(X) - I(X;Y)\)</span> since <span class="math inline">\(I(X;Y)\)</span>
is the information you can learn about <span
class="math inline">\(X\)</span> from learning about <span
class="math inline">\(Y\)</span>.</p></li>
<li><p>This diagram shows the additive relationships between different
entropy related quantities of two variables. <img
src="additive_entropy.png" alt="image" /> From which you can also deduce
for example that <span class="math display">\[I(X;Y) = H(X,Y) - H(X|Y) -
H(Y|X)\]</span></p></li>
</ul>
</body>
</html>
